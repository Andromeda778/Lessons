{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3e6a3109493126ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e6a3109493126ba",
        "outputId": "014d2d53-312e-4ae8-e6ec-f7a950ed8efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d478ffc9940c2f2b",
      "metadata": {
        "id": "d478ffc9940c2f2b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import torchtext\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab, Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import io\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67f302b5405b566",
      "metadata": {
        "collapsed": false,
        "id": "d67f302b5405b566"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d184f09af780b80",
      "metadata": {
        "collapsed": false,
        "id": "1d184f09af780b80"
      },
      "source": [
        "## Загрузка и предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "initial_id",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "a9509ddc-aa98-4aa9-e6f5-e50a9086f54c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Определяются URL-ы для загрузки обучающих, валидационных и тестовых данных\n",
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "# cкачивание и распаковка данных.\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "# Создаем токенизаторы для немецкого и английского языков\n",
        "de_tokenizer = get_tokenizer('spacy', language='de')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "# Создаем словари для немецкого и английского языков\n",
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      counter.update(tokenizer(string_))\n",
        "  v = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "  v.set_default_index(counter['<unk>'])\n",
        "  return v\n",
        "\n",
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
        "\n",
        "# Определяем функцию data_process, которая преобразует данные в тензоры PyTorch. Эта функция принимает пути к файлам с данными, считывает данные, токенизирует каждое предложение и преобразует токены в индексы с помощью словаря. Результаты сохраняются в список тензоров.\n",
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
        "                            dtype=torch.long)\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
        "                            dtype=torch.long)\n",
        "    data.append((de_tensor_, en_tensor_))\n",
        "  return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3a1efce87652ae82",
      "metadata": {
        "id": "3a1efce87652ae82"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Определяем параметры для обучения, такие как размер пакета (BATCH_SIZE) и индексы специальных токенов (PAD_IDX, BOS_IDX, EOS_IDX).\n",
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "\n",
        "# Определяем функцию generate_batch, которая генерирует пакеты данных для обучения. Эта функция принимает пакет данных, добавляет специальные токены в начало и конец каждого предложения и выполняет дополнение предложений до одинаковой длины.\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e9ee63a89786ac9",
      "metadata": {
        "collapsed": false,
        "id": "6e9ee63a89786ac9"
      },
      "source": [
        "# Реализация модели\n",
        "Здесь надо реализовать методы forward для Encoder, Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9cbec98d7f018571",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cbec98d7f018571",
        "outputId": "19f7cf15-172f-4d0f-ad1e-9923969618c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 1,353,142 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # ваш код\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # ваш код\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n",
        "\n",
        "# Задайте параметры\n",
        "\n",
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "HID_DIM = 32\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5480b054daa63f5",
      "metadata": {
        "collapsed": false,
        "id": "5480b054daa63f5"
      },
      "source": [
        "# Обучение модели\n",
        "Здесб надо дописать функции train и evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2306bed616924b62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2306bed616924b62",
        "outputId": "9ca9662b-ba61-44ba-c85d-ffea3daa01a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30, 128]) torch.Size([27, 128])\n",
            "torch.Size([37, 128]) torch.Size([44, 128])\n",
            "torch.Size([28, 128]) torch.Size([30, 128])\n",
            "torch.Size([25, 128]) torch.Size([27, 128])\n",
            "torch.Size([27, 128]) torch.Size([33, 128])\n",
            "torch.Size([28, 128]) torch.Size([33, 128])\n",
            "torch.Size([33, 128]) torch.Size([34, 128])\n",
            "torch.Size([30, 128]) torch.Size([28, 128])\n",
            "torch.Size([27, 128]) torch.Size([27, 128])\n",
            "torch.Size([37, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([30, 128])\n",
            "torch.Size([30, 128]) torch.Size([34, 128])\n",
            "torch.Size([33, 128]) torch.Size([35, 128])\n",
            "torch.Size([30, 128]) torch.Size([27, 128])\n",
            "torch.Size([31, 128]) torch.Size([29, 128])\n",
            "torch.Size([27, 128]) torch.Size([26, 128])\n",
            "torch.Size([34, 128]) torch.Size([32, 128])\n",
            "torch.Size([46, 128]) torch.Size([43, 128])\n",
            "torch.Size([28, 128]) torch.Size([30, 128])\n",
            "torch.Size([32, 128]) torch.Size([31, 128])\n",
            "torch.Size([28, 128]) torch.Size([33, 128])\n",
            "torch.Size([28, 128]) torch.Size([32, 128])\n",
            "torch.Size([37, 128]) torch.Size([33, 128])\n",
            "torch.Size([27, 128]) torch.Size([30, 128])\n",
            "torch.Size([28, 128]) torch.Size([32, 128])\n",
            "torch.Size([35, 128]) torch.Size([34, 128])\n",
            "torch.Size([24, 128]) torch.Size([23, 128])\n",
            "torch.Size([33, 128]) torch.Size([31, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([22, 128]) torch.Size([24, 128])\n",
            "torch.Size([31, 128]) torch.Size([29, 128])\n",
            "torch.Size([24, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([30, 128])\n",
            "torch.Size([30, 128]) torch.Size([26, 128])\n",
            "torch.Size([34, 128]) torch.Size([32, 128])\n",
            "torch.Size([33, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([35, 128])\n",
            "torch.Size([29, 128]) torch.Size([33, 128])\n",
            "torch.Size([34, 128]) torch.Size([36, 128])\n",
            "torch.Size([34, 128]) torch.Size([40, 128])\n",
            "torch.Size([28, 128]) torch.Size([33, 128])\n",
            "torch.Size([27, 128]) torch.Size([30, 128])\n",
            "torch.Size([36, 128]) torch.Size([41, 128])\n",
            "torch.Size([37, 128]) torch.Size([38, 128])\n",
            "torch.Size([26, 128]) torch.Size([28, 128])\n",
            "torch.Size([29, 128]) torch.Size([30, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([34, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([34, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([31, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([31, 128]) torch.Size([30, 128])\n",
            "torch.Size([34, 128]) torch.Size([35, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([32, 128]) torch.Size([32, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([36, 128]) torch.Size([36, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([30, 128])\n",
            "torch.Size([32, 128]) torch.Size([34, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([24, 128]) torch.Size([24, 128])\n",
            "torch.Size([26, 128]) torch.Size([30, 128])\n",
            "torch.Size([37, 128]) torch.Size([28, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([35, 128]) torch.Size([31, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([35, 128]) torch.Size([38, 128])\n",
            "torch.Size([33, 128]) torch.Size([32, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([31, 128]) torch.Size([30, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([27, 128]) torch.Size([28, 128])\n",
            "torch.Size([31, 128]) torch.Size([30, 128])\n",
            "torch.Size([35, 128]) torch.Size([41, 128])\n",
            "torch.Size([26, 128]) torch.Size([29, 128])\n",
            "torch.Size([27, 128]) torch.Size([29, 128])\n",
            "torch.Size([25, 128]) torch.Size([29, 128])\n",
            "torch.Size([32, 128]) torch.Size([30, 128])\n",
            "torch.Size([39, 128]) torch.Size([37, 128])\n",
            "torch.Size([30, 128]) torch.Size([28, 128])\n",
            "torch.Size([24, 128]) torch.Size([28, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([33, 128]) torch.Size([29, 128])\n",
            "torch.Size([31, 128]) torch.Size([35, 128])\n",
            "torch.Size([38, 128]) torch.Size([37, 128])\n",
            "torch.Size([30, 128]) torch.Size([32, 128])\n",
            "torch.Size([39, 128]) torch.Size([37, 128])\n",
            "torch.Size([30, 128]) torch.Size([38, 128])\n",
            "torch.Size([32, 128]) torch.Size([39, 128])\n",
            "torch.Size([31, 128]) torch.Size([28, 128])\n",
            "torch.Size([30, 128]) torch.Size([35, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([31, 128]) torch.Size([32, 128])\n",
            "torch.Size([26, 128]) torch.Size([29, 128])\n",
            "torch.Size([31, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([31, 128])\n",
            "torch.Size([32, 128]) torch.Size([36, 128])\n",
            "torch.Size([35, 128]) torch.Size([36, 128])\n",
            "torch.Size([30, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([33, 128])\n",
            "torch.Size([32, 128]) torch.Size([34, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([34, 128]) torch.Size([31, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([31, 128]) torch.Size([38, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([42, 128]) torch.Size([43, 128])\n",
            "torch.Size([30, 128]) torch.Size([34, 128])\n",
            "torch.Size([31, 128]) torch.Size([27, 128])\n",
            "torch.Size([26, 128]) torch.Size([29, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([34, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([27, 128]) torch.Size([27, 128])\n",
            "torch.Size([33, 128]) torch.Size([35, 128])\n",
            "torch.Size([28, 128]) torch.Size([30, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([31, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([26, 128])\n",
            "torch.Size([35, 128]) torch.Size([42, 128])\n",
            "torch.Size([37, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([34, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([33, 128]) torch.Size([36, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([26, 128]) torch.Size([30, 128])\n",
            "torch.Size([30, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([29, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([27, 128]) torch.Size([28, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([27, 128]) torch.Size([30, 128])\n",
            "torch.Size([27, 128]) torch.Size([30, 128])\n",
            "torch.Size([35, 128]) torch.Size([33, 128])\n",
            "torch.Size([25, 128]) torch.Size([27, 128])\n",
            "torch.Size([28, 128]) torch.Size([26, 128])\n",
            "torch.Size([32, 128]) torch.Size([30, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([32, 128]) torch.Size([31, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([31, 128]) torch.Size([32, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([32, 128]) torch.Size([28, 128])\n",
            "torch.Size([32, 128]) torch.Size([30, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([30, 128]) torch.Size([27, 128])\n",
            "torch.Size([25, 128]) torch.Size([26, 128])\n",
            "torch.Size([28, 128]) torch.Size([24, 128])\n",
            "torch.Size([28, 128]) torch.Size([32, 128])\n",
            "torch.Size([38, 128]) torch.Size([37, 128])\n",
            "torch.Size([34, 128]) torch.Size([30, 128])\n",
            "torch.Size([29, 128]) torch.Size([35, 128])\n",
            "torch.Size([35, 128]) torch.Size([32, 128])\n",
            "torch.Size([34, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([27, 128]) torch.Size([26, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([36, 128]) torch.Size([33, 128])\n",
            "torch.Size([34, 128]) torch.Size([37, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([38, 128]) torch.Size([41, 128])\n",
            "torch.Size([34, 128]) torch.Size([36, 128])\n",
            "torch.Size([29, 128]) torch.Size([26, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([36, 128]) torch.Size([38, 128])\n",
            "torch.Size([26, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([33, 128])\n",
            "torch.Size([25, 128]) torch.Size([28, 128])\n",
            "torch.Size([36, 128]) torch.Size([34, 128])\n",
            "torch.Size([24, 128]) torch.Size([25, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([27, 128]) torch.Size([26, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([30, 128]) torch.Size([27, 128])\n",
            "torch.Size([28, 128]) torch.Size([26, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([25, 128]) torch.Size([25, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([31, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([29, 128]) torch.Size([33, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([32, 128]) torch.Size([29, 128])\n",
            "torch.Size([32, 128]) torch.Size([39, 128])\n",
            "torch.Size([37, 128]) torch.Size([39, 128])\n",
            "torch.Size([30, 128]) torch.Size([33, 128])\n",
            "torch.Size([31, 128]) torch.Size([32, 128])\n",
            "torch.Size([32, 128]) torch.Size([34, 128])\n",
            "torch.Size([47, 128]) torch.Size([38, 128])\n",
            "torch.Size([27, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([37, 128])\n",
            "torch.Size([38, 128]) torch.Size([37, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([37, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([33, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([26, 128])\n",
            "torch.Size([46, 128]) torch.Size([42, 128])\n",
            "torch.Size([26, 128]) torch.Size([30, 128])\n",
            "torch.Size([25, 128]) torch.Size([26, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([34, 128]) torch.Size([36, 128])\n",
            "torch.Size([31, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([29, 128])\n",
            "torch.Size([27, 128]) torch.Size([31, 128])\n",
            "torch.Size([26, 128]) torch.Size([31, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([35, 128]) torch.Size([34, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([38, 128]) torch.Size([36, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([28, 72]) torch.Size([27, 72])\n",
            "Epoch: 01 | Time: 0m 54s\n",
            "\tTrain Loss: 6.360 | Train PPL: 577.979\n",
            "\t Val. Loss: 5.503 |  Val. PPL: 245.312\n",
            "torch.Size([27, 128]) torch.Size([29, 128])\n",
            "torch.Size([28, 128]) torch.Size([30, 128])\n",
            "torch.Size([34, 128]) torch.Size([35, 128])\n",
            "torch.Size([31, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([32, 128]) torch.Size([32, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([25, 128]) torch.Size([24, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([27, 128]) torch.Size([35, 128])\n",
            "torch.Size([30, 128]) torch.Size([33, 128])\n",
            "torch.Size([30, 128]) torch.Size([28, 128])\n",
            "torch.Size([30, 128]) torch.Size([26, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([27, 128]) torch.Size([27, 128])\n",
            "torch.Size([26, 128]) torch.Size([27, 128])\n",
            "torch.Size([34, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([34, 128]) torch.Size([36, 128])\n",
            "torch.Size([38, 128]) torch.Size([35, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([38, 128]) torch.Size([36, 128])\n",
            "torch.Size([34, 128]) torch.Size([36, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([34, 128]) torch.Size([30, 128])\n",
            "torch.Size([27, 128]) torch.Size([32, 128])\n",
            "torch.Size([37, 128]) torch.Size([32, 128])\n",
            "torch.Size([28, 128]) torch.Size([26, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([35, 128])\n",
            "torch.Size([28, 128]) torch.Size([32, 128])\n",
            "torch.Size([33, 128]) torch.Size([32, 128])\n",
            "torch.Size([33, 128]) torch.Size([39, 128])\n",
            "torch.Size([28, 128]) torch.Size([27, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([25, 128])\n",
            "torch.Size([26, 128]) torch.Size([30, 128])\n",
            "torch.Size([30, 128]) torch.Size([37, 128])\n",
            "torch.Size([34, 128]) torch.Size([32, 128])\n",
            "torch.Size([28, 128]) torch.Size([27, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([24, 128]) torch.Size([30, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([31, 128]) torch.Size([34, 128])\n",
            "torch.Size([34, 128]) torch.Size([29, 128])\n",
            "torch.Size([35, 128]) torch.Size([33, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([31, 128]) torch.Size([30, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([35, 128]) torch.Size([34, 128])\n",
            "torch.Size([30, 128]) torch.Size([28, 128])\n",
            "torch.Size([27, 128]) torch.Size([25, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([35, 128]) torch.Size([34, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([37, 128]) torch.Size([32, 128])\n",
            "torch.Size([33, 128]) torch.Size([29, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([32, 128]) torch.Size([33, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([27, 128]) torch.Size([29, 128])\n",
            "torch.Size([38, 128]) torch.Size([37, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([38, 128]) torch.Size([41, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([37, 128]) torch.Size([33, 128])\n",
            "torch.Size([31, 128]) torch.Size([34, 128])\n",
            "torch.Size([32, 128]) torch.Size([30, 128])\n",
            "torch.Size([28, 128]) torch.Size([30, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([46, 128]) torch.Size([44, 128])\n",
            "torch.Size([34, 128]) torch.Size([29, 128])\n",
            "torch.Size([33, 128]) torch.Size([34, 128])\n",
            "torch.Size([38, 128]) torch.Size([37, 128])\n",
            "torch.Size([26, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([34, 128])\n",
            "torch.Size([24, 128]) torch.Size([27, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([29, 128]) torch.Size([30, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([27, 128]) torch.Size([26, 128])\n",
            "torch.Size([29, 128]) torch.Size([33, 128])\n",
            "torch.Size([25, 128]) torch.Size([33, 128])\n",
            "torch.Size([33, 128]) torch.Size([31, 128])\n",
            "torch.Size([27, 128]) torch.Size([27, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([39, 128]) torch.Size([37, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([31, 128]) torch.Size([27, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([36, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([27, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([27, 128]) torch.Size([27, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([31, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([31, 128])\n",
            "torch.Size([31, 128]) torch.Size([34, 128])\n",
            "torch.Size([37, 128]) torch.Size([38, 128])\n",
            "torch.Size([29, 128]) torch.Size([34, 128])\n",
            "torch.Size([25, 128]) torch.Size([28, 128])\n",
            "torch.Size([47, 128]) torch.Size([38, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([34, 128]) torch.Size([40, 128])\n",
            "torch.Size([31, 128]) torch.Size([29, 128])\n",
            "torch.Size([27, 128]) torch.Size([31, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([28, 128]) torch.Size([27, 128])\n",
            "torch.Size([29, 128]) torch.Size([30, 128])\n",
            "torch.Size([31, 128]) torch.Size([32, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([33, 128])\n",
            "torch.Size([32, 128]) torch.Size([36, 128])\n",
            "torch.Size([31, 128]) torch.Size([39, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([34, 128]) torch.Size([36, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([32, 128]) torch.Size([32, 128])\n",
            "torch.Size([32, 128]) torch.Size([29, 128])\n",
            "torch.Size([38, 128]) torch.Size([31, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([26, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([26, 128])\n",
            "torch.Size([37, 128]) torch.Size([30, 128])\n",
            "torch.Size([33, 128]) torch.Size([28, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([31, 128]) torch.Size([38, 128])\n",
            "torch.Size([26, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([34, 128])\n",
            "torch.Size([33, 128]) torch.Size([35, 128])\n",
            "torch.Size([34, 128]) torch.Size([29, 128])\n",
            "torch.Size([34, 128]) torch.Size([36, 128])\n",
            "torch.Size([26, 128]) torch.Size([27, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([34, 128]) torch.Size([31, 128])\n",
            "torch.Size([27, 128]) torch.Size([30, 128])\n",
            "torch.Size([26, 128]) torch.Size([26, 128])\n",
            "torch.Size([36, 128]) torch.Size([36, 128])\n",
            "torch.Size([27, 128]) torch.Size([28, 128])\n",
            "torch.Size([28, 128]) torch.Size([32, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([39, 128]) torch.Size([37, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([35, 128]) torch.Size([31, 128])\n",
            "torch.Size([31, 128]) torch.Size([32, 128])\n",
            "torch.Size([28, 128]) torch.Size([29, 128])\n",
            "torch.Size([30, 128]) torch.Size([32, 128])\n",
            "torch.Size([37, 128]) torch.Size([38, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([32, 128]) torch.Size([30, 128])\n",
            "torch.Size([30, 128]) torch.Size([35, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([24, 128]) torch.Size([30, 128])\n",
            "torch.Size([37, 128]) torch.Size([39, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([29, 128]) torch.Size([30, 128])\n",
            "torch.Size([29, 128]) torch.Size([32, 128])\n",
            "torch.Size([27, 128]) torch.Size([26, 128])\n",
            "torch.Size([22, 128]) torch.Size([24, 128])\n",
            "torch.Size([31, 128]) torch.Size([30, 128])\n",
            "torch.Size([34, 128]) torch.Size([30, 128])\n",
            "torch.Size([30, 128]) torch.Size([29, 128])\n",
            "torch.Size([35, 128]) torch.Size([36, 128])\n",
            "torch.Size([26, 128]) torch.Size([31, 128])\n",
            "torch.Size([46, 128]) torch.Size([42, 128])\n",
            "torch.Size([39, 128]) torch.Size([38, 128])\n",
            "torch.Size([32, 128]) torch.Size([27, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([29, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([30, 128]) torch.Size([32, 128])\n",
            "torch.Size([35, 128]) torch.Size([32, 128])\n",
            "torch.Size([33, 128]) torch.Size([31, 128])\n",
            "torch.Size([31, 128]) torch.Size([31, 128])\n",
            "torch.Size([25, 128]) torch.Size([30, 128])\n",
            "torch.Size([25, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([28, 128])\n",
            "torch.Size([27, 128]) torch.Size([28, 128])\n",
            "torch.Size([30, 128]) torch.Size([30, 128])\n",
            "torch.Size([33, 128]) torch.Size([34, 128])\n",
            "torch.Size([30, 128]) torch.Size([28, 128])\n",
            "torch.Size([32, 128]) torch.Size([28, 128])\n",
            "torch.Size([33, 128]) torch.Size([35, 128])\n",
            "torch.Size([31, 128]) torch.Size([28, 128])\n",
            "torch.Size([32, 128]) torch.Size([30, 128])\n",
            "torch.Size([27, 128]) torch.Size([27, 128])\n",
            "torch.Size([38, 128]) torch.Size([41, 128])\n",
            "torch.Size([35, 128]) torch.Size([42, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([24, 128]) torch.Size([33, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([28, 128]) torch.Size([28, 128])\n",
            "torch.Size([32, 128]) torch.Size([33, 128])\n",
            "torch.Size([32, 128]) torch.Size([37, 128])\n",
            "torch.Size([28, 128]) torch.Size([30, 128])\n",
            "torch.Size([32, 128]) torch.Size([33, 128])\n",
            "torch.Size([26, 128]) torch.Size([32, 128])\n",
            "torch.Size([42, 128]) torch.Size([43, 128])\n",
            "torch.Size([26, 128]) torch.Size([29, 128])\n",
            "torch.Size([26, 128]) torch.Size([27, 128])\n",
            "torch.Size([27, 128]) torch.Size([29, 128])\n",
            "torch.Size([36, 128]) torch.Size([41, 128])\n",
            "torch.Size([31, 128]) torch.Size([35, 128])\n",
            "torch.Size([27, 128]) torch.Size([29, 128])\n",
            "torch.Size([29, 128]) torch.Size([27, 128])\n",
            "torch.Size([30, 128]) torch.Size([33, 128])\n",
            "torch.Size([35, 128]) torch.Size([41, 128])\n",
            "torch.Size([32, 128]) torch.Size([35, 128])\n",
            "torch.Size([29, 128]) torch.Size([31, 128])\n",
            "torch.Size([33, 128]) torch.Size([33, 128])\n",
            "torch.Size([24, 128]) torch.Size([28, 128])\n",
            "torch.Size([29, 128]) torch.Size([30, 128])\n",
            "torch.Size([25, 72]) torch.Size([28, 72])\n",
            "Epoch: 02 | Time: 0m 53s\n",
            "\tTrain Loss: 5.502 | Train PPL: 245.183\n",
            "\t Val. Loss: 5.507 |  Val. PPL: 246.493\n",
            "| Test Loss: 5.495 | Test PPL: 243.400 |\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "def train(model: nn.Module,\n",
        "          iterator: torch.utils.data.DataLoader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "        # написать шаг\n",
        "        src_i = src.to(device)\n",
        "        trg_i = trg.to(device)\n",
        "        print(src.size(), trg.size())\n",
        "        # src = [src length, batch size]\n",
        "        # trg = [trg length, batch size]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src_i, trg_i)\n",
        "        # output = [trg length, batch size, trg vocab size]\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
        "        trg_i = trg_i[1:].view(-1)\n",
        "        # trg = [(trg length - 1) * batch size]\n",
        "        loss = criterion(output, trg_i)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (src, trg) in enumerate(iterator):\n",
        "            # написать шаг\n",
        "            src_i = src.to(device)\n",
        "            trg_i = trg.to(device)\n",
        "            output = model(src_i, trg_i, 0)  # turn off teacher forcing\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg_i = trg_i[1:].view(-1)\n",
        "            loss = criterion(output, trg_i)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "PAD_IDX = en_vocab.get_stoi()['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "\n",
        "N_EPOCHS = 2\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5d3dcec88bc586",
      "metadata": {
        "collapsed": false,
        "id": "ae5d3dcec88bc586"
      },
      "source": [
        "# Трансформер\n",
        "Здесь также надо реализовать метод forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "94e2eefef39f7dc8",
      "metadata": {
        "id": "94e2eefef39f7dc8"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, emb_dim, nhead, hid_dim, nlayers, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(emb_dim, dropout)\n",
        "        self.encoder = nn.Embedding(input_dim, emb_dim)\n",
        "        self.decoder = nn.Embedding(output_dim, emb_dim)\n",
        "        self.transformer = Transformer(d_model=emb_dim, nhead=nhead, num_encoder_layers=nlayers, num_decoder_layers=nlayers, dim_feedforward=hid_dim, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Инициализация параметров\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.emb_dim)\n",
        "        src = self.pos_encoder(src)\n",
        "        trg = self.encoder(trg) * math.sqrt(self.emb_dim)\n",
        "        trg = self.pos_encoder(trg)\n",
        "        output = self.transformer(src, trg, src_mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.5, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7964055289395b4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7964055289395b4b",
        "outputId": "65c7948a-1def-4f9b-8b4c-6d1393aefb3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "EMB_DIM = 256\n",
        "NHEAD = 8\n",
        "HID_DIM = 512\n",
        "NLAYERS = 3\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = TransformerModel(INPUT_DIM, OUTPUT_DIM, EMB_DIM, NHEAD, HID_DIM, NLAYERS, DROPOUT).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9f0f189851325a31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9f0f189851325a31",
        "outputId": "0b02c311-d3fd-478a-976d-c3a7e54ef8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([29, 128]) torch.Size([28, 128])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c6fc21589341>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-057f23593aa7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# trg = [trg length, batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# output = [trg length, batch size, trg vocab size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-b2dd9f34f9aa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 2\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion, \"transformer\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion, \"transformer\")\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
